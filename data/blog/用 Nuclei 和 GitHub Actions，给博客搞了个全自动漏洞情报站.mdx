---
title: 用 Nuclei 和 GitHub Actions，给博客搞了个全自动漏洞情报站
date: '2026-01-09'
tags: ['Nuclei', '安全情报', 'CVE POC']
draft: false
summary: 用 Nuclei + GitHub Actions 搭一套免费的自动化漏洞情报流水线：每天抓取 nuclei-templates 更新，按 CVE/风险打分筛选高价值 POC，自动生成结构化 MDX 文章并以 PR 形式发布到博客。
images: ['/static/images/image_1768099621278624.png']
category: 'app-vuln'
---

## 前言

作为WAF运营，我每天都在“欣赏”全球扫描器的7x24小时“问候”。时间久了，我就一个问题：攻击者手里的“新武器”都是从哪来的？

答案直指 projectdiscovery/nuclei-templates —— 全球白帽子的“军火”贡献库。

一个念头随之而来：如果我能抢在攻击者之前，洞悉这个“军火库”的每次更新，我的WAF策略不就能做到“知己知彼”了吗？

于是，我花了几个晚上，用 Nuclei 和 GitHub Actions 搞了个全自动的“漏洞情报站”。

它每天自动分析最新的高价值POC，整理成报告发布出来。这东西现在不仅是我WAF运营的“千里眼”，也成了一个向所有开发者预警最新威胁的“烽火台”。

今天，我把它的完整思路和代码分享出来，希望能帮助大家从被动防御，转向主动洞察。

## 方案与建设

**工具箱：就这两样，简单粗暴**

1. **Nuclei**: 这玩意儿不用多介绍，搞安全的都知道。ProjectDiscovery 出品，一个字，快。关键是它背后那个 `nuclei-templates` 模板库，全世界的“白帽子”都在给它贡献弹药，更新速度比新闻还快。
2. **GitHub Actions**: 就是你仓库里自带的免费CI/CD。你可以把它当成一个24小时待命的Linux小弟，让它帮你跑各种脚本。

把这两样东西捏在一起，就能实现我们的目标：**让机器人每天定时干活，分析最新的漏洞，然后写成博客。**

整个系统的所有秘密，都在代码里。

### **1. 搭工作流 (The Workflow YAML)**

这步是在告诉你项目里的 `.github/workflows/nuclei-poc-analysis.yml` 文件是干啥的。

yaml

```yaml
name: Nuclei POC Daily Analysis

on:
  schedule:
# 每天早上9点（UTC时间1点）爬起来干活
    - cron:'0 1 * * *'
  workflow_dispatch: {}# 加个按钮，方便我随时手动踹它一脚让它干活

jobs:
  nuclei-analysis:
    runs-on: ubuntu-latest
    permissions:# 坑点1：必须给权限，不然机器人没法帮你提交代码
      contents: write
      pull-requests: write
    steps:
# ...检出代码、装环境...

      - name: 🔍 核心步骤：跑分析脚本
        env:
          GITHUB_TOKEN:${{ secrets.GITHUB_TOKEN }}
          ANALYSIS_HOURS:${{ github.event.inputs.hours || '24' }}
          MAX_TEMPLATES: 30# 省着点用，API调用次数有限
        run: |
echo"开工了，分析一波Nuclei POC..."
          node scripts/nuclei-poc-analyzer.js

# ...后面就是检查有没有新文章生成，有的话就提交、创建PR...

```

**稍微解释一下：**

- `on`: 告诉机器人啥时候干活。`cron: '0 1 * * *'` 就是每天UTC时间凌晨1点（北京时间早上9点）准时开工。
- `permissions`: **这里有一个坑**。一开始忘了加，结果机器人跑完活想提交代码，直接被GitHub权限系统拒绝了。这里是授权它替你提交代码和创建PR。
- `Run Nuclei POC Analysis`: 这就是整个流程的心脏。它调用了下面要说的 `nuclei-poc-analyzer.js` 脚本，把情报处理逻辑的活具交给它处理。

### **2. 自动分析逻辑“大脑” (The Node.js Script)**

如果只是简单跑一下Nuclei，那信息量能把你淹死。我们需要的是**情报**，不是**噪音**。

所以，我用Node.js写了一个 `nuclei-poc-analyzer.js` 脚本，它的任务不是“执行”，而是“思考”。

**它的思考逻辑是这样的：**

1. **只看最新的**: 它先用GitHub API去 `projectdiscovery/nuclei-templates` 仓库把最近24小时更新的POC内容都抓下来。
2. **筛选高危漏洞**: 漏洞POC千千万，只把严重/高危的漏洞挑出来，这样看起来就更清晰。

   JavaScript

   ```jsx
   // 思路大概是这样
   functionfilterHighValueTemplates(templates){
   const priorities = {
   'cves/':10,// CVE漏洞，必须是大哥
   'vulnerabilities/':8,// 通用漏洞，也很重要
   'exposures/':6,// 信息泄露，看看
   // ...其他技术识别、面板之类的，优先级靠后...
     };

   // ...根据路径、是不是新增等因素加权打分，然后从高到低排序...
   }

   ```

   为啥要这么干？因为GitHub API的调用次数是有限的，我可不想把宝贵的次数浪费在一些无关痛痒的技术识别模板上。**把好钢用在刀刃上**，优先分析那些带CVE编号的、高危的、新提交的漏洞。

3. **榨干情报**: 对挑出来的“狠货”，脚本会把它们榨干，提取所有有价值的信息：
   - **基础信息**：漏洞名叫啥，谁写的，危害多大。
   - **风险评估**：不只看`severity`，还结合CVSS分数，算一个更客观的风险分。
   - **影响谁**：根据`tags`和`path`猜一下，这漏洞是打Nginx的，还是搞WordPress的？影响面有多大？

### **3.整理分析结果，自动发文**

分析完了，最后一步就是把这些机器码一样的情报，变成人能看懂的文章。

脚本会把分析结果套进一个预设的MDX模板里，自动生成标题、摘要、标签，把漏洞详情、风险评级、受影响资产都安排得明明白白，最后在文章末尾附上可以直接复制粘贴的Nuclei扫描命令。

一篇排版良好、信息量十足的情报分析文章，就这么诞生了。

**看看效果：机器人写的文章长这样**

最终，你每天早上起来，就能在博客上看到一篇类似这样的新文章：

> 标题：Nuclei POC 精选分析 - 2026-01-09摘要：从45个模板更新中精选分析30个高价值POC，发现5个高风险漏洞。重点漏洞分析### Example Corp RCE 漏洞漏洞ID: example-corp-rce-2026风险等级: 极高风险 (4.8/5)影响资产: Example Corp Server, API 接口描述: ...（此处是人话描述）扫描建议bash# 直接复制就能用
> nuclei -t cves/2026/CVE-2026-12345.yaml -u your-target.com

**一些废话（但很有用）**

- **省着点用API**: GitHub API是有限的，我设置了 `MAX_TEMPLATES: 30` 就是为了防止玩脱了被限流。智能筛选算法是这里的关键。
- **给自己一个Review的机会**: 我让机器人在每周一自动创建一个PR，而不是直接合并。这样我能快速过一眼这周的产出，万一有啥离谱的东西，还能拦一下。
- **失败了要知道**: Action可能会因为网络、API等玄学问题失败。我加了个`health-check`任务，失败了会输出一些提示，方便我快速排查。

## **结语**

就这么个东西。说白了，就是把人肉做的、重复性的信息搜集和分析工作，扔给机器去干。

它不仅让我自己的项目安全感大增，还顺便给我的博客带来源源不断的、有价值的内容。

我觉得，这才是咱们技术人打造影响力该干的事：**发现一个问题，用技术漂亮地解决它，然后分享出来。**

代码都在我的博客和GitHub上，随便拿去用，改成你喜欢的样子。如果你用它搞出了更有意思的东西，记得回来告诉我。
